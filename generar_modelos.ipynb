{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudialeguiza/AA1-TUIA-Kidonakis-Leguiza/blob/navegador/generar_modelos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "cLXc7T2zEqU4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.pipeline import Pipeline\n",
        "import keras\n",
        "from keras.models import Sequential, save_model, load_model\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import Precision\n",
        "from keras.utils import to_categorical\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datos = pd.read_csv('/content/weatherAUS.csv', delimiter = \",\")"
      ],
      "metadata": {
        "id": "-30WTKMoFk2h"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = datos[datos.Location\\\n",
        "                      .isin(( 'Sydney','SydneyAirport','Melbourne', 'MelbourneAirport',\\\n",
        "                             'Canberra','Adelaide', 'MountGambier','Cobar', 'Dartmoor' ))]"
      ],
      "metadata": {
        "id": "D8UxDqY7GAQf"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocesamiento(data):\n",
        "    data.info()\n",
        "    data.isna().sum()\n",
        "\n",
        "    # Definir columnas con valores nulos\n",
        "    columnas_con_nulos = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',\n",
        "                          'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm','Humidity9am',\n",
        "                          'Humidity3pm', 'Pressure9am','Pressure3pm', 'Cloud9am',\n",
        "                          'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainfallTomorrow']\n",
        "\n",
        "    # Rellenar valores faltantes en 'RainToday' y 'RainTomorrow'\n",
        "    data['RainToday'] = data.groupby('Date')['RainToday'].transform(lambda x: x.fillna(x.mode().iloc[0]))\n",
        "    data['RainTomorrow'] = data.groupby('Date')['RainTomorrow'].transform(lambda x: x.fillna(x.mode().iloc[0]))\n",
        "\n",
        "    # Rellenar valores faltantes en direcciones del viento\n",
        "    data['WindGustDir'] = data.groupby('Date')['WindGustDir'].transform(lambda x: x.fillna(x.mode().iloc[0]) if not x.isna().all() else x)\n",
        "    data['WindDir9am'] = data.groupby('Date')['WindDir9am'].transform(lambda x: x.fillna(x.mode().iloc[0]) if not x.isna().all() else x)\n",
        "    data['WindDir3pm'] = data.groupby('Date')['WindDir3pm'].transform(lambda x: x.fillna(x.mode().iloc[0]) if not x.isna().all() else x)\n",
        "\n",
        "    # Rellenar valores faltantes con la media por día para las columnas especificadas\n",
        "    media_por_dia = data.groupby('Date')[columnas_con_nulos].transform('mean')\n",
        "    data[columnas_con_nulos] = data[columnas_con_nulos].fillna(media_por_dia)\n",
        "\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "Vm6gEs76HBKk"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_columna_season(data):\n",
        "   data['season'] = data['Date'].apply(asignar_estacion)\n",
        "   return data"
      ],
      "metadata": {
        "id": "BJx1V3tHHViG"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def asignar_estacion(fecha):\n",
        "    mes = fecha.month\n",
        "    if mes in [12, 1, 2]:  # Verano: Diciembre, Enero, Febrero\n",
        "        return 'Summer'\n",
        "    elif mes in [3, 4, 5]:  # Otoño: Marzo, Abril, Mayo\n",
        "        return 'Autumn'\n",
        "    elif mes in [6, 7, 8]:  # Invierno: Junio, Julio, Agosto\n",
        "        return 'Winter'\n",
        "    else:  # Primavera: Septiembre, Octubre, Noviembre\n",
        "        return 'Spring'"
      ],
      "metadata": {
        "id": "25SCfZasHepN"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def codificar_variables(data):\n",
        "    data1 = pd.get_dummies(data, columns=['RainToday', 'RainTomorrow', 'season', 'Location'], drop_first=True)\n",
        "\n",
        "    # Crear columnas para WindGustDir, WindDir9am, WindDir3pm\n",
        "    wind_directions = [\"SW\", \"S\", 'SSW', 'W', 'SSE', 'E', 'SE', 'NE', 'NNE', 'WSW', 'WNW', 'NW', 'N', 'ESE', 'ENE']\n",
        "    for var in wind_directions:\n",
        "        data1[f'WindGustDir_{var}'] = (data['WindGustDir'] == var).astype(int)\n",
        "        data1[f'WindDir9am_{var}'] = (data['WindDir9am'] == var).astype(int)\n",
        "        data1[f'WindDir3pm_{var}'] = (data['WindDir3pm'] == var).astype(int)\n",
        "\n",
        "    return data1.drop(columns=['WindGustDir', 'WindDir9am', 'WindDir3pm'])"
      ],
      "metadata": {
        "id": "JngxP-xIHjFl"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estandarizar_df(data):\n",
        "  scaler = RobustScaler()\n",
        "  data_scaled = scaler.fit_transform(data)\n",
        "  return data_scaled"
      ],
      "metadata": {
        "id": "py6hmsCWWbUi"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def truncar_dividir_df(data):\n",
        "    data = data.sort_values([\"Date\"])\n",
        "    fecha_especifica = '2009-01-01'\n",
        "    data_filtrada = data[data['Date'] >= fecha_especifica]\n",
        "\n",
        "    data_filtrada.reset_index(drop=True, inplace=True)  # Resetea el índice y no crea uno nuevo\n",
        "    data_train = data_filtrada.iloc[:21658]\n",
        "\n",
        "    return data_train"
      ],
      "metadata": {
        "id": "t4iH0DupLSY8"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estandarizar_df(data):\n",
        "    # Separar variables independientes y dependientes\n",
        "    X_regresion = data.drop(columns =['RainfallTomorrow', 'RainTomorrow_Yes','Date'])\n",
        "    X_scaled = estandarizar_df(data)\n",
        "    y_regresion = data['RainfallTomorrow'].values.reshape(-1, 1)\n",
        "    y_scaled = estandarizar_df(pd.DataFrame(y_regresion, columns=['RainfallTomorrow']))\n",
        "    return X_scaled, y_scaled"
      ],
      "metadata": {
        "id": "zJk2EMejqT5l"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estandarizar_balancear_clas(data):\n",
        "    #X_clasificacion = data.drop(columns=['RainTomorrow_Yes'])\n",
        "    X_scaled1 = estandarizar_df(data)\n",
        "    y_clasificacion = data['RainTomorrow_Yes']\n",
        "\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_smote_scaled, y_smote_scaled = smote.fit_resample(X_scaled1, y_clasificacion)\n",
        "\n",
        "    return X_smote_scaled, y_smote_scaled"
      ],
      "metadata": {
        "id": "Uto6RXcgcruv"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la arquitectura de la red neuronal para regresión\n",
        "def create_regression_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(126, input_dim=input_shape, activation='relu'))\n",
        "    model.add(Dropout(0.5)) # Capa de Dropout para regularizacion, evita el overfitting\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1))  # Capa de salida para regresión\n",
        "    return model"
      ],
      "metadata": {
        "id": "SyNLlM6yoUnw"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regression_model(X_train_scaled, y_train_scaled):\n",
        "  # Crear el modelo\n",
        "  regression_model = create_regression_model(X_train_scaled.shape[1])\n",
        "\n",
        "  # Compilar el modelo\n",
        "  regression_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "  # Entrenar el modelo\n",
        "  history_regression = regression_model.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=32)\n",
        "  return"
      ],
      "metadata": {
        "id": "O-_zgTkTodBQ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la arquitectura de la red neuronal para clasificación\n",
        "def create_classification_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(126, input_dim=input_shape, activation='relu'))\n",
        "    model.add(Dropout(0.5)) # Capa de Dropout para regularizacion, evita el overfitting\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5)) # Capa de Dropout para regularizacion, evita el overfitting\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Capa de salida para clasificación binaria\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "DvUEoVfLvEH7"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clasification_model(X_smote_train, y_smote_train):\n",
        "  # Crear el modelo\n",
        "  classification_model = create_classification_model(X_smote_train.shape[1])\n",
        "\n",
        "  # Compilar el modelo\n",
        "  classification_model.compile(optimizer=Adam(learning_rate=0.001), loss= 'binary_crossentropy', metrics= ['Precision'])\n",
        "\n",
        "  # Entrenar el modelo\n",
        "  history_classification = classification_model.fit(X_smote_train, y_smote_train, epochs=100, batch_size=32)\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "vgRAz7SFvK3r"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_prepara_datos = Pipeline([\n",
        "    ('preproceso', FunctionTransformer(preprocesamiento, validate=False)),\n",
        "    ('season', FunctionTransformer(crear_columna_season, validate=False)),\n",
        "    ('codificar', FunctionTransformer(codificar_variables, validate=False))\n",
        "])\n",
        "\n",
        "# Obtener datos de entrenamiento\n",
        "df_procesado = pipeline_prepara_datos.fit_transform(df)\n",
        "\n",
        "pipeline_train_split = Pipeline([\n",
        "    ('split', FunctionTransformer(truncar_dividir_df, validate=False)),\n",
        "    ('estandarizar', FunctionTransformer(estandarizar_df, validate=False)),\n",
        "    ])\n",
        "\n",
        "pipeline_modelo_regresion = Pipeline([\n",
        "     ('modelo', FunctionTransformer(regression_model, validate=False))\n",
        "                                      ])\n",
        "\n",
        "# Obtener datos de entrenamiento\n",
        "X_train_scaled, y_train_scaled = pipeline_train_split.fit_transform(df_procesado)\n",
        "\n",
        " # Entrenar el modelo\n",
        "pipeline_modelo_regresion.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=32)\n",
        "\n",
        "joblib.dump(pipeline_modelo_regresion, 'models/regresion_pipeline.joblib')"
      ],
      "metadata": {
        "id": "QXESzQ93GB1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_train_split_clas = Pipeline([\n",
        "    ('split', FunctionTransformer(truncar_dividir_df, validate=False)),\n",
        "    ('estandarizar_clas', FunctionTransformer(estandarizar_balancear_clas, validate=False)),\n",
        "    ])\n",
        "\n",
        "pipeline_modelo_clasificacion = Pipeline([\n",
        "     ('modelo_clas', FunctionTransformer(clasification_model, validate=False))\n",
        "                                      ])\n",
        "\n",
        "df_procesado1 = pipeline_prepara_datos.fit_transform(df)\n",
        "\n",
        "# Obtener datos de entrenamiento\n",
        "X_smote_train, y_smote_train = pipeline_train_split_clas.fit_transform(df_procesado1)\n",
        "\n",
        "# Entrenar_modelo\n",
        "pipeline_modelo_clasificacion.fit(X_smote_train, y_smote_train, epochs=100, batch_size=32)\n",
        "\n",
        "joblib.dump(pipeline_modelo_clasificacion, 'models/clasificacion_pipeline.joblib')"
      ],
      "metadata": {
        "id": "emoswcDTGW0G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
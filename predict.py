# -*- coding: utf-8 -*-
"""predict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13m8YStypW_PVPuvW1dKSsEkGZG1spaub

<a href="https://colab.research.google.com/github/claudialeguiza/AA1-TUIA-Kidonakis-Leguiza/blob/navegador/predict.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import RobustScaler, FunctionTransformer
from imblearn.over_sampling import SMOTE
from sklearn.pipeline import Pipeline
import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.optimizers import Adam
from keras.metrics import Precision
from keras.utils import to_categorical
import joblib
import warnings
warnings.simplefilter('ignore')

df = pd.read_csv('weatherAUS.csv', delimiter = ",")

def preprocesamiento(datos):

    data = datos[datos.Location\
                      .isin(( 'Sydney','SydneyAirport','Melbourne', 'MelbourneAirport',\
                             'Canberra','Adelaide', 'MountGambier','Cobar', 'Dartmoor' ))]


    data = data.drop('Unnamed: 0', axis =1)

    # Definir columnas con valores nulos
    columnas_con_nulos = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',
                          'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm','Humidity9am',
                          'Humidity3pm', 'Pressure9am','Pressure3pm', 'Cloud9am',
                          'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainfallTomorrow']

    # Rellenar valores faltantes en 'RainToday' y 'RainTomorrow'
    data['RainToday'] = data.groupby('Date')['RainToday'].transform(lambda x: x.fillna(x.mode().iloc[0]))
    data['RainTomorrow'] = data.groupby('Date')['RainTomorrow'].transform(lambda x: x.fillna(x.mode().iloc[0]))

    # Rellenar valores faltantes en direcciones del viento
    data['WindGustDir'] = data.groupby('Date')['WindGustDir'].transform(lambda x: x.fillna(x.mode().iloc[0]) if not x.isna().all() else x)
    data['WindDir9am'] = data.groupby('Date')['WindDir9am'].transform(lambda x: x.fillna(x.mode().iloc[0]) if not x.isna().all() else x)
    data['WindDir3pm'] = data.groupby('Date')['WindDir3pm'].transform(lambda x: x.fillna(x.mode().iloc[0]) if not x.isna().all() else x)

    # Rellenar valores faltantes con la media por día para las columnas especificadas
    media_por_dia = data.groupby('Date')[columnas_con_nulos].transform('mean')
    data[columnas_con_nulos] = data[columnas_con_nulos].fillna(media_por_dia)

    data['Date'] = pd.to_datetime(data['Date'])

    return data

def crear_columna_season(data):
   data['season'] = data['Date'].apply(asignar_estacion)
   return data

def asignar_estacion(fecha):
    mes = fecha.month
    if mes in [12, 1, 2]:  # Verano: Diciembre, Enero, Febrero
        return 'Summer'
    elif mes in [3, 4, 5]:  # Otoño: Marzo, Abril, Mayo
        return 'Autumn'
    elif mes in [6, 7, 8]:  # Invierno: Junio, Julio, Agosto
        return 'Winter'
    else:  # Primavera: Septiembre, Octubre, Noviembre
        return 'Spring'

def codificar_variables(data):
    data1 = pd.get_dummies(data, columns=['RainToday', 'RainTomorrow','season', 'Location'], drop_first=True)

    # Crear columnas para WindGustDir, WindDir9am, WindDir3pm
    wind_directions = ["SW", "S", 'SSW', 'W', 'SSE', 'E', 'SE', 'NE', 'NNE', 'WSW', 'WNW', 'NW', 'N', 'ESE', 'ENE']
    for var in wind_directions:
        data1[f'WindGustDir_{var}'] = (data['WindGustDir'] == var).astype(int)
        data1[f'WindDir9am_{var}'] = (data['WindDir9am'] == var).astype(int)
        data1[f'WindDir3pm_{var}'] = (data['WindDir3pm'] == var).astype(int)

    return data1.drop(columns=['WindGustDir', 'WindDir9am', 'WindDir3pm'])

def truncar_dividir_df(data):
    data = data.sort_values(["Date"])
    fecha_especifica = '2009-01-01'
    data_filtrada = data[data['Date'] >= fecha_especifica]

    data_filtrada.reset_index(drop=True, inplace=True)  # Resetea el índice y no crea uno nuevo
    data_train = data_filtrada.iloc[:21658]
    return data_train

def eliminar_columnas(data):
    # Separar variables independientes y dependientes
    X_regresion = data.drop(columns =['RainfallTomorrow','Date', 'RainTomorrow_Yes'])
    y_regresion = data['RainfallTomorrow']
    y_regresion = y_regresion.values.reshape(-1,1)
    return X_regresion, y_regresion

def balancear_clas(data):
    X_clasificacion = data.drop(columns=['RainTomorrow_Yes','Date','RainfallTomorrow'])
    y_clasificacion = data['RainTomorrow_Yes']
    y_clasificacion = y_clasificacion.values.reshape(-1,1)
    smote = SMOTE(random_state=42)
    X_smote, y_smote = smote.fit_resample(X_clasificacion, y_clasificacion)

    return X_smote, y_smote

# Definir la arquitectura de la red neuronal para regresión
def create_regression_model(input_shape):
    model = Sequential()
    model.add(Dense(126, input_dim=input_shape, activation='relu'))
    model.add(Dropout(0.5)) # Capa de Dropout para regularizacion, evita el overfitting
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(32, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1))  # Capa de salida para regresión
    return model

# Definir la función para crear el modelo de clasificación con los mejores hiperparámetros
def create_classification_model_with_params(input_shape, best_params):
    model = Sequential()
    model.add(Dense(best_params['n_units_layer_0'], input_dim=input_shape, activation='relu'))  # Capa de entrada
    for i in range(1, best_params['num_layers']):
        model.add(Dense(best_params[f'n_units_layer_{i}'], activation='relu'))  # Capas ocultas
    model.add(Dense(1, activation='sigmoid'))  # Capa de salida para clasificación binaria
    return model

pipeline_prepara_datos = Pipeline([
    ('preproceso', FunctionTransformer(preprocesamiento, validate=False)),
    ('season', FunctionTransformer(crear_columna_season, validate=False)),
    ('codificar', FunctionTransformer(codificar_variables, validate=False))
])

df_procesado = pipeline_prepara_datos.fit_transform(df)

pipeline_train_split_regresion = Pipeline([
    ('split', FunctionTransformer(truncar_dividir_df, validate=False)),
    ('eliminar', FunctionTransformer(eliminar_columnas, validate=False)),
    ])

pipeline_train_split_clas = Pipeline([
    ('split', FunctionTransformer(truncar_dividir_df, validate=False)),
    ('balancear_clas', FunctionTransformer(balancear_clas, validate=False)),
    ])

X_train, y_train = pipeline_train_split_regresion.fit_transform(df_procesado)
X_smote, y_smote = pipeline_train_split_clas.fit_transform(df_procesado)

# Crear el modelo regresion
modelo_regresion = create_regression_model(X_train.shape[1])

# Crear el modelo de clasificación con los mejores hiperparámetros
best_params = {'num_layers': 3,
               'n_units_layer_0': 105,
               'n_units_layer_1': 104,
               'n_units_layer_2': 94,
               'learning_rate': 0.02944312151129532,
               'batch_size': 16}

modelo_clasificacion=\
         create_classification_model_with_params(X_smote.shape[1], best_params)

# Compilar el modelo de regresion
modelo_regresion.compile(optimizer=Adam(learning_rate=0.001), loss='mse',\
                         metrics=['mae'])

# Compilar el modelo de clasificacion
modelo_clasificacion.compile(optimizer=Adam(learning_rate=0.001),\
                              loss='binary_crossentropy', metrics=['Precision'])


pipeline_regresion =Pipeline([('scaler', RobustScaler()),
                              ('regresion', modelo_regresion),
                               ])

pipeline_clasificacion = Pipeline([('scaler', RobustScaler()),
              ('clasificacion', modelo_clasificacion)
                                 ])

pipeline_regresion.fit(X_train, y_train, regresion__epochs=100, regresion__batch_size=32)
joblib.dump(pipeline_regresion, 'regresion_clima.pkl')

pipeline_clasificacion.fit(X_smote, y_smote, clasificacion__epochs=100, clasificacion__batch_size = 16)
joblib.dump(pipeline_clasificacion, 'clasificacion_clima.pkl')